[{"content":"Edit (2023-01-20): This post got a lot of attention, so I want to clarify some things first. I\u0026rsquo;m a reverse engineering noob who hates EA with a passion. This blog post contains my personal, highly biased, opinions. If you want an unbiased description of the encryption used by EA Desktop, you can visit the GameFinder Wiki.\nBackground Before I go in-depth on the encryption, I want to start by explaining the background of all of this:\nI\u0026rsquo;m the developer of GameFinder, a .NET library for finding games installed via various stores. The library supports Steam, GOG Galaxy, the Epic Games Store, Origin and now also EA Desktop. In case you haven\u0026rsquo;t seen the news, EA is deprecating Origin and replacing it with their new program: \u0026ldquo;EA Desktop\u0026rdquo; or \u0026ldquo;EA App\u0026rdquo;. The old way of finding games installed via Origin doesn\u0026rsquo;t work for EA Desktop. This blog post will detail my 4-day journey of figuring out how I can support EA Desktop, which ultimately led to me reverse engineering and breaking its pathetic encryption.\nStart of the journey All of this began when I wanted to add support for EA Desktop. Origin had previously used manifest files to store information about installed games. These manifest files were located in C:\\Program Data\\Origin\\LocalContent, however EA Desktop doesn\u0026rsquo;t create those files anymore.\nI started by looking around in C:\\Program Data and found a folder named EA Desktop:\nC:. │ machine.ini │ ├───530c11479fe252fc5aabc24935b9776d4900eb3ba58fdc271e0d6229413ad40e │ CATS │ IQ │ IS │ IS.json │ ├───6d13902b1570caf2c578d1f15fb1d5d9a2e62426246bc5eebea5707931ecdc21 │ CS │ IQ │ ├───InstallData │ └───Apex │ └───base-Origin.SFT.50.0000848 │ └───Logs EABackgroundService.log We have some very ominous looking folder names, some installation data and a couple log files. However, notice that IS.json file? This is what started this rabbit hole, because it looks like this (you can view the full file here):\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 { \u0026#34;installInfos\u0026#34;: [ { \u0026#34;baseInstallPath\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;baseSlug\u0026#34;: \u0026#34;plants-vs-zombies-garden-warfare-2\u0026#34;, \u0026#34;softwareId\u0026#34;: \u0026#34;Origin.SFT.50.0000345\u0026#34; }, { \u0026#34;baseInstallPath\u0026#34;: \u0026#34;E:\\\\SteamLibrary\\\\steamapps\\\\common\\\\Titanfall2\\\\\u0026#34;, \u0026#34;baseSlug\u0026#34;: \u0026#34;titanfall-2\u0026#34;, \u0026#34;softwareId\u0026#34;: \u0026#34;Origin.SFT.50.0000532\u0026#34; }, { \u0026#34;baseInstallPath\u0026#34;: \u0026#34;M:\\\\Games\\\\new EA App is shit\\\\Apex\\\\\u0026#34;, \u0026#34;baseSlug\u0026#34;: \u0026#34;apex-legends\u0026#34;, \u0026#34;softwareId\u0026#34;: \u0026#34;Origin.SFT.50.0000848\u0026#34; }, { \u0026#34;baseInstallPath\u0026#34;: \u0026#34;M:\\\\SteamLibrary\\\\steamapps\\\\common\\\\Need for Speed Heat\\\\\u0026#34;, \u0026#34;baseSlug\u0026#34;: \u0026#34;need-for-speed-heat\u0026#34;, \u0026#34;softwareId\u0026#34;: \u0026#34;Origin.SFT.50.0001079\u0026#34; }, { \u0026#34;baseInstallPath\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;baseSlug\u0026#34;: \u0026#34;need-for-speed-hot-pursuit-remastered\u0026#34;, \u0026#34;softwareId\u0026#34;: \u0026#34;Origin.SFT.50.0001232\u0026#34; } ], \u0026#34;schema\u0026#34;: { \u0026#34;version\u0026#34;: 21 } } When I found this file, I thought I was done already. This file contains everything I need for the library: the installation path, some identifier and the name of the game. I quickly wrote my implementation and send a debug build of my library to some testers that came back with this error message:\nUnable to find IS.json inside data folder C:\\ProgramData\\EA Desktop Turns out I\u0026rsquo;m the only one that has this IS.json file. Looking at the file dates, I noticed that it was created in early December 2022 and never touched again. I\u0026rsquo;m guessing some earlier version of EA Desktop created this file, but not anymore. Instead, it creates a file called IS in the same folder.\nAnalysis with CyberChef Note: I will be using CyberChef throughout this blog post, if you want to follow along, you can download my encrypted file from GitHub and load it into CyberChef.\nAt this point I had no idea what I\u0026rsquo;m dealing with, so I grabbed the IS file and just looked at the Hexdump:\nHexdump of the first 240 bytes\nThe file starts out with some kind of 256-bit hash, written as plaintext and the rest is just an incoherent collection of bytes. The Shannon Entropy is a good measure to identify if the input is structured or unstructured, and applying it on everything after the hash results in a value of 7.965:\nVisualization of the Shannon Scale on the input\nIt\u0026rsquo;s safe to assume the contents are encrypted. Now we only have to figure out what algorithm and key were used.\nTrying out everything At this point I couldn\u0026rsquo;t do much without doing some reverse engineering. But before I can open up my favorite tool, I had to figure out what process even creates this file. Thankfully the Sysinternals Tools are amazing as always and provide the Process Monitor tool for monitoring file activity in real-time.\nI noticed that the changed date of the encrypted file changes when I start or stop a download. Realizing this, I prepared a big download of Apex Legends, limited the download speed to 512kb/s and started monitoring with Process Monitor using this filter:\nAdding a path filter to Process Monitor\nWith this filter activated, Process Monitor will only display entries where the path points to the encrypted file. After starting the monitoring and resuming and pausing the Apex Legends download a couple of times, I got these results:\nProcess Monitor results\nThe process EABackgroundService.exe seems to be the only one that accesses those files. What\u0026rsquo;s even more impressive with Process Monitor, is the fact that you can double-click any of these entries and view the stack:\nStack of the first WriteFile entry\nI now had everything I needed: the encrypted file, the decrypted file, the process that creates the file and multiple locations I can investigate.\nDebugging and Reverse Engineering Identifying the Algorithm Note: I\u0026rsquo;m writing this blog post on 2023-01-18, the file version of EABackgroundService.exe is 12.85.0.5342 and its SHA256 hash is ba93d7a3a406128df5adc8f32322a897196eea83a6976b409dbd0e5401a612c8.\nWith the stack information from Process Monitor, I could start diving into the assembly code of the process. I used x64dbg to attach and debug the process and Ghidra to reverse engineering compiled code.\nI started out by going up the call stack and looking at each function in Ghidra. The first couple functions were just wrapper functions around CreateFileW, WriteFile and other file system related stuff. However, the fourth function at EABackgroundService+0x271644 was a hit. Not only is this function massive, it references some interesting strings:\nSaving [{}] into file: \\n[{}] eax::services::localStorage::encryptDataToFile I don\u0026rsquo;t think you can get more obvious than this. Turns out that excessive logging with inlined strings are really helpful.\nI found an interesting function, now I only had to figure out what it does and where the encryption is being done. For this I don\u0026rsquo;t have any neat tricks up my sleeve, I just put a breakpoint at the start of the function in x64dbg and went through it step by step while also looking at the same code in Ghidra. I was mostly interested in the function parameters being passed, so I kept my eye on the RCX, RDX, R8 and R9 registers which are being used to pass parameters for the x64 calling convention.\nIt didn\u0026rsquo;t take me long to find a very interesting call at 0x270f26 with these parameters:\nRCX: address of some region in memory RDX: address of the plaintext R8: address to 01B42F0E7E3B32E7C4251BC38FA2AE2EDB8DC26498E5B73E2A92AC9E8FFCB4F4 R9: address to 84EFC4B836119C20419398C3F3F2BCEF6FC52F9D86C6E4E8756AEC5A8279E492 This was very suspicious. After opening the function in Ghidra, I was very overjoyed to find this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 // before cleanup uVar2 = EVP_aes_256_cbc(); iVar1 = FUN_345be0(local_c8, param_2, param_3, param_4, uVar2, local_e8); if (iVar1 \u0026lt; 1) { FUN_54e50(\u0026amp;local_60, \u0026#34;AES256 CBC encryption failed\u0026#34;, 0x1c); } // after cleanup cipherType = EVP_aes_256_cbc(); ok = Encrypt(local_c8, plaintext, key, iv, cipherType, encryptedOut); if (!ok) { Log(\u0026amp;local_60, \u0026#34;AES256 CBC encryption failed\u0026#34;, 0x1c); } Turns out that EA Desktop is using OpenSSL\u0026rsquo;s EVP functions to encrypt their files. In this case the file we need is encrypted using AES with a key length of 256 bits using the Cipher Block Chaining (CBC) mode.\nUsing x64dbg, I just dumped the Key and IV from memory and used CyberChef to successfully decrypt the file:\nSuccessful decryption in CyberChef\nRecreating the IV At this point I was very happy. I managed to identify the algorithm being used and was able to decrypt the file using the Key and IV from memory. The next step was figuring out how I can recreate them. You have to remember that my goal is to implement a method in my library, that can decrypt the file on its own, without requiring the consumer of the library to do anything special.\nI started by following the parameters being passed to the encryption function. These values have to come from somewhere and I intend to find out where. Once again, I\u0026rsquo;m using x64dbg to step through the function and look closely at the registers. Now that I know what the Key and IV look like, I can identify the functions that create them or at least reference them.\nAnd just as luck would have it, I found a function before the encryption function that accepts some curious parameters and outputs the Key and IV:\nRCX: address of some region in memory RDX: address of the string allUsersGenericId R8: address of the string IS Once again I investigated and after a bit of cleanup, the function essentially does this:\n1 2 3 4 CreateCryptographicHash(cryptographicHash, 0x100); AddDataToHash(cryptographicHash, param_2); AddDataToHash(cryptographicHash, param_3); ok = FinalizeHash(cryptographicHash, \u0026amp;finalHashOut); These are wrapper functions for QCryptographicHash from Qt 5. CreateCryptographicHash is a wrapper for the constructor QCryptographicHash(QCryptographicHash::Algorithm method) where they always pass SHA3 256 as the algorithm and AddDataToHash is a wrapper for addData(const char *data, int length).\nIf you\u0026rsquo;re like me, and you see this, then you quickly head over to CyberChef, input allUsersGenericIdIS and create the SHA3 256 hash to get THE IV as output: 84efc4b836119c20419398c3f3f2bcef6fc52f9d86c6e4e8756aec5a8279e492.\nTHE IV NEVER CHANGES: It\u0026rsquo;s always SHA3_256(\u0026quot;allUsersGenericId\u0026quot; + \u0026quot;IS\u0026quot;), a fucking constant.\nAs a quick side note: remember 530c11479fe252fc5aabc24935b9776d4900eb3ba58fdc271e0d6229413ad40e? This is the folder name of the file IS. This is also a SHA3 256 hash (CyberChef):\nSHA3_256(\u0026#34;allUsersGenericId\u0026#34;) = 530c11479fe252fc5aabc24935b9776d4900eb3ba58fdc271e0d6229413ad40e I have no idea where this allUsersGenericId comes from, but some genius at EA thought it would be a good idea to include this in almost every hash.\nRecreating the Key Anyways, we have the IV. Now the remaining part is figuring out how to recreate the Key, which turned out to be more involved than the IV.\nThe function that creates IV did some more stuff afterwards. They reset the SHA3 256 hash instance and added some very interesting data to it: allUsersGenericIdISa2a0ad25aa3556c035b34ea63863794e54ad5b53 (CyberChef)\nSHA3_256(\u0026#34;allUsersGenericIdISa2a0ad25aa3556c035b34ea63863794e54ad5b53\u0026#34;) = 01b42f0e7e3b32e7c4251bc38fa2ae2edb8dc26498e5b73e2a92ac9e8ffcb4f4 This is the key we previously dumped from memory. Let\u0026rsquo;s break this down: \u0026quot;allUsersGenericId\u0026quot; + \u0026quot;IS\u0026quot; + \u0026quot;a2a0ad25aa3556c035b34ea63863794e54ad5b53\u0026quot;. The first two parts are already known to us, they are the components of the IV. The last part a2a0ad25aa3556c035b34ea63863794e54ad5b53 is more interesting. This looks like another hash, but it only has a length of 160 bits. Once again we can use CyberChef to analyze this hash:\nHash length: 40 Byte length: 20 Bit length: 160 Based on the length, this hash could have been generated by one of the following hashing functions: SHA-1 SHA-0 FSB-160 HAS-160 HAVAL-160 RIPEMD-160 Tiger-160 Based on the previous results, I\u0026rsquo;m assuming this is a SHA1 hash, since that\u0026rsquo;s the only one that OpenSSL supports.\nEdit (2023-01-20): Someone on reddit informed me that OpenSSL also supports RIPEMD. My statement was a bit misleading, as SHA1 was the only algorithm from that list that was imported from OpenSSL, not supported. I just looked at the imports in Ghidra and found that SHA1 was the only match, so I went with it, and it turned out to be correct.\nWith this information in hand, I decided to try going the opposite way and start with the import of EVP_sha1 and look at what references this. Turns out there is only one function that uses this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ulonglong HashSomething(undefined8* param_1, undefined8 param_2, int param_3, undefined8* param_4, int param_5) { switch (param_5 - 2) { //... case 3: algorithm = EVP_sha1(); break; //... } ctx = EVP_MD_CTX_new(); ok = EVP_DigsteInit_ex(ctx, algorithm, nullptr); EVP_DigestUpdate(ctx, param_2, param_3); EVP_DigestFinal_ex(ctx, *param_4, \u0026amp;param_5); EVP_MD_CTX_free(ctx); } I aptly named this function HashSomething as it appears to be some sort of general purpose hashing function that takes some input, the output buffer and probably an enum to control which hashing algorithm should be used. This looked very promising, so I went back to x64dbg, put a breakpoint at the start of the function and inspected the input, only to find this:\n1 ASRock; ;American Megatrends Inc.;To Be Filled By O.E.M.;7CB7433E;PCI\\VEN_10DE\u0026amp;DEV_2486\u0026amp;SUBSYS_147A10DE\u0026amp;REV_A1\\4\u0026amp;2283F625\u0026amp;0\u0026amp;0019;AuthenticAMD;178BFBFF00A20F10;AMD Ryzen 7 5800X 8-Core Processor ; Using CyberChef to verify my assumption, I was very happy to find out that this is the missing component:\nSHA1(hardwareInfo) = a2a0ad25aa3556c035b34ea63863794e54ad5b53 They hash the hardware info using SHA1, then combine that hash with the constants allGenericId and IS to create the Key.\nHardware Information In order to replicate the hardware info hash, I needed to figure out which components and which properties went into it and where they are getting the information from. On Windows, the best way to do this, is to use the WMI. I\u0026rsquo;ve never used the WMI, so I just checked out an example to find the library functions I need to look out for. These include CoInitialize, CoCreateInstance and CoSetProxyBlanket, just to name a few. With that newly found information, I went back into Ghidra and went up the call stack of the general purpose hashing function to find something that looks good.\nI ended up in a function that was being called by the function that creates the IV and Key. However, I noticed that there was a big fat static initialization being done. Turns out that they collect your hardware information when the process starts and then never again. Using x64dbg to restart the process and I stepped through the static initialization to figure out what was going on.\nAfter a rather tedious process of continuously looking at register values until something made sense, I managed to extract every class and property name used to create the hardware info:\nWin32_BaseBoard Manufacturer = ASRock Win32_BaseBoard SerialNumber = (string with lots of whitespace) Win32_BIOS Manufacturer = American Megatrends Inc. Win32_BIOS SerialNumber = To Be Filled By O.E.M. Win32_VideoController PNPDeviceId = PCI\\VEN_10DE\u0026amp;DEV_2486\u0026amp;SUBSYS_147A10DE\u0026amp;REV_A1\\4\u0026amp;2283F625\u0026amp;0\u0026amp;0019 Win32_Processor Manufacturer = AuthenticAMD Win32_Processor ProcessorId = 178BFBFF00A20F10 Win32_Processor Name = AMD Ryzen 7 5800X 8-Core Processor (also had a lot of whitespace at the end) You can try this out yourself by using the wmic tool:\n1 wmic PATH Win32_BaseBoard get Manufacturer But we\u0026rsquo;re not done yet. There is one value missing: 7CB7433E. This is the hex value of the serial number of the volume associated with the root directory of your C:\\ drive. It is returned by GetVolumeInformationW. Note that this a different serial number from the one obtained by Win32_PhysicalMedia SerialNumber:\nThis function returns the volume serial number that the operating system assigns when a hard disk is formatted. To programmatically obtain the hard disk\u0026rsquo;s serial number that the manufacturer assigns, use the Windows Management Instrumentation (WMI) Win32_PhysicalMedia property SerialNumber.\nConclusion graph TD allUsersGenericId \u0026 IS --\u003e allUsersGenericIdIS[allUsersGenericId + IS] hardwareInfo[Hardware Information] --\u003e |SHA1| hardwareInfoHash[Hardware Info Hash] allUsersGenericIdIS \u0026 hardwareInfoHash --\u003e combine[allUsersGenericId + IS + Hardware Info Hash] --\u003e |SHA3 256| KEY allUsersGenericIdIS --\u003e |SHA3 256| IV This flowchart shows the complete process of generating the Key and IV. With those values, you can easily decrypt the file using AES 256 CBC.\nLet\u0026rsquo;s go back to the point of all of this: finding out which games are installed. I did all of this just to decrypt a file that contains some installation paths to EA games. I don\u0026rsquo;t understand why this file is encrypted in the first place. It doesn\u0026rsquo;t make any sense. You encrypt a file because it contains sensitive information and/or you want to keep it from prying eyes. Now the question is \u0026ldquo;who is not supposed to read this file\u0026rdquo;? The fact that they created the plaintext version of this file at some point but later changed it on purpose means they don\u0026rsquo;t want anyone to read it.\nAnd this isn\u0026rsquo;t even all, if you look at the flowchart, you will realize that if the user changes a single hardware component of their PC, the Key will be different, and you won\u0026rsquo;t be able to decrypt the file anymore. The team at EA that implemented this and the person that made the decision to encrypt the file in the first place, have no idea what they are doing. This is a pathetic attempt to prevent users from reading this file.\nI have demonstrated how easy it is to \u0026ldquo;break\u0026rdquo; the encryption using tools like CyberChef, x64dbg and Ghidra. The \u0026ldquo;encryption\u0026rdquo; used by EA Desktop is straight up pathetic and useless. In the off chance that anyone from the team that works on the program is reading this: just write the plaintext file, there is no point in encrypting it.\nFor me personally, this has been a very fun adventure. I managed to make good progress every day and got more experience using the tools listed above and assembly in general. I can also now finally have support for EA Desktop in my GameFinder library, so users can stop pinging me about it.\n","permalink":"https://erri120.github.io/posts/2023-01-18/","summary":"EA made a sad attempt to prevent me from reading their files. I\u0026rsquo;ll explain how I went about breaking their encryption.","title":"Breaking EA Desktop's pathetic Encryption"},{"content":"Following my post on Character encodings and Unicode it is now time to talk about i18n with GNU gettext. We will look at i18n and l10n in general and then talk about how gettext can make our live as programmers very easy.\ni18n and l10n - Internationalization and Localization Due to the length of Internationalization and Localization you can just write i18n and l10n which are \u0026ldquo;numeronyms\u0026rdquo;, number based words that are formed by taking the first and last character of the words and putting the amount of letters between these two characters in the middle, so for Internationalization it starts with an i and ends with an n and has 18 letters in-between resulting in i18n.\nFor us software developers these terms mean adapting our code to be locale agnostic. If you create a UI and hard-code all strings then the users won\u0026rsquo;t be able to change the language. Aside from normal translations i18n and l10n also encompasses formatting rules for numbers, date and time, currency and things like text layout. Some languages read left to right, others right to left. Instead of reading horizontally there are also cultures where you read vertically.\nAll of this might seem overwhelming and in reality you will likely never have to deal with this. For Open-Source projects it\u0026rsquo;s often enough to just have everything in English and maybe provide a way to load translations.\nGNU gettext and libintl i18n and l10n is all nice and good, but how should we programmers design our software to support these concepts? This is where gettext and libintl come into play.\nIn 1995 the GNU projects released GNU gettext into the world. The package offers an integrated set of tools as well as the libintl runtime library for dealing with translations. We will take a look at the tools xgettext, msginit, msgmerge and msgfmt, how to use the gettext library in our code and what the process of creating translations is.\nCMake Setup In order for us to use libintl in our code we need to get it from somewhere. In this example we will use CMake and vcpkg.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 cmake_minimum_required(VERSION 3.8) project ( \u0026#34;CppInternationalization\u0026#34; VERSION 1.0.0 LANGUAGES CXX ) # Using C++20 set(CMAKE_CXX_STANDARD 20) set(CMAKE_CXX_STANDARD_REQUIRED ON) set(CMAKE_CXX_EXTENSIONS OFF) add_executable(${PROJECT_NAME} \u0026#34;main.cpp\u0026#34;) # add libintl find_package(Intl REQUIRED) target_link_libraries(${PROJECT_NAME} PUBLIC ${Intl_LIBRARY}) target_include_directories(${PROJECT_NAME} PUBLIC ${Intl_INCLUDE_DIRS}) This is a very basic CMakeLists.txt file containing one dependency which we will get with vcpkg using vcpkg install gettext[tools]. The tools feature is very important so we can get the programs required for our setup.\nCode Setup 1 2 3 4 5 6 7 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; int main() { std::cout \u0026lt;\u0026lt; \u0026#34;Hello World!\u0026#34; \u0026lt;\u0026lt; std::endl; return EXIT_SUCCESS; } This is the most basic C++ program possible. In the code we hard-coded the string Hello World!, and now we want to provide translations. The libintl runtime library has exactly what we need:\n1 2 3 4 5 6 7 8 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;libintl.h\u0026gt; int main() { std::cout \u0026lt;\u0026lt; gettext(\u0026#34;Hello World!\u0026#34;) \u0026lt;\u0026lt; std::endl; return EXIT_SUCCESS; } The gettext function from the libintl.h header will now look for a translation of the string Hello World! for the current locale at runtime. If it does not find a translation it will just use Hello World! which is very nice since we only have to wrap all the strings in gettext(). If gettext is too long you can create a macro:\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;libintl.h\u0026gt; #define _(String) gettext(String) int main() { std::cout \u0026lt;\u0026lt; _(\u0026#34;Hello World!\u0026#34;) \u0026lt;\u0026lt; std::endl; return EXIT_SUCCESS; } This macro is very commonly used in projects that use GNU gettext and even included in frameworks like GTK.\nSo where does gettext() look for my translations? You will have to specify that with bindtextdomain:\n1 2 3 4 5 6 7 8 9 10 11 12 13 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;libintl.h\u0026gt; #define _(String) gettext(String) int main() { bindtextdomain(\u0026#34;my-domain\u0026#34;, \u0026#34;locales\u0026#34;); textdomain(\u0026#34;my-domain\u0026#34;); std::cout \u0026lt;\u0026lt; _(\u0026#34;Hello World!\u0026#34;) \u0026lt;\u0026lt; std::endl; return EXIT_SUCCESS; } Now at runtime if the user has the de locale set, gettext() will look at locales/de/LC_MESSAGES/my-domain.mo for a translation. Let\u0026rsquo;s break this path down:\nlocales: the folder specified in bindtextdomain de: the current locale LC_MESSAGES: the category name of the translation my-domain.mo: the binary message catalog containing all translations. The file name is what you specified in bindtextdomain as well as textdomain There are few things that need explaining. The library will first try and find an exact match of the current locale but if that is not possible it will look for similar locales. As an example if the user has de_DE, but you only provided de then the library will first look for de_DE and when it doesn\u0026rsquo;t find it, it will look for an expanded locale like de.\nThe LC_MESSAGES part of the path is the category name of the translation we are looking for. LC stands for locale category and there are various others like LC_CTYPE, LC_NUMERIC, LC_TIME, LC_MONETARY which all specify how to handle various things like numbers, dates and currency. For our purposes we only focus on messages, raw strings or texts we want to translate. gettext will always use LC_MESSAGES as its category. There are other functions that will let you specify which category you want to look for like dcgettext but for this post we won\u0026rsquo;t look at those.\nThe .mo file is the message catalog which has to be generated. So let\u0026rsquo;s look at how that works next.\nInitial Project Setup In our very complex example we have marked the Hello World! string as a translatable string. We can now use xgettext to extract these marked strings:\n1 xgettext main.cpp --keyword=\u0026#34;_\u0026#34; --output=\u0026#34;locales/my-domain.pot\u0026#34; This .pot file is a Portable Object Template file. It can look like this:\n# SOME DESCRIPTIVE TITLE. # FIRST AUTHOR \u0026lt;EMAIL@ADDRESS\u0026gt;, YEAR. # #, fuzzy msgid \u0026#34;\u0026#34; msgstr \u0026#34;\u0026#34; \u0026#34;POT-Creation-Date: 2022-05-05 14:47+0200\\n\u0026#34; \u0026#34;PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\\n\u0026#34; \u0026#34;Last-Translator: FULL NAME \u0026lt;EMAIL@ADDRESS\u0026gt;\\n\u0026#34; \u0026#34;Language-Team: LANGUAGE \u0026lt;LL@li.org\u0026gt;\\n\u0026#34; \u0026#34;Language: \\n\u0026#34; \u0026#34;MIME-Version: 1.0\\n\u0026#34; \u0026#34;Content-Type: text/plain; charset=CHARSET\\n\u0026#34; \u0026#34;Content-Transfer-Encoding: 8bit\\n\u0026#34; #: main.cpp:10 msgid \u0026#34;Hello World!\u0026#34; msgstr \u0026#34;\u0026#34; At the bottom you can find our Hello World! string which comes from main.cpp at line 10. This file can now be used to create .po, Portable Object files using msginit:\n1 msginit --input=\u0026#34;locales/my-domain.pot\u0026#34; --output-file=\u0026#34;locales/de/my-domain.po\u0026#34; --locale=\u0026#34;de\u0026#34; Every language you want to support gets its own .po file. The .pot is just a template used to create the .po files with. The .po file looks almost the same:\nmsgid \u0026#34;\u0026#34; msgstr \u0026#34;\u0026#34; \u0026#34;POT-Creation-Date: 2022-05-05 14:47+0200\\n\u0026#34; \u0026#34;PO-Revision-Date: 2022-05-04 18:41+0200\\n\u0026#34; \u0026#34;Last-Translator: \u0026lt;EMAIL@ADDRESS\u0026gt;\\n\u0026#34; \u0026#34;Language-Team: German\\n\u0026#34; \u0026#34;Language: de\\n\u0026#34; \u0026#34;MIME-Version: 1.0\\n\u0026#34; \u0026#34;Content-Type: text/plain; charset=CP1252\\n\u0026#34; \u0026#34;Content-Transfer-Encoding: 8bit\\n\u0026#34; \u0026#34;Plural-Forms: nplurals=2; plural=(n != 1);\\n\u0026#34; \u0026#34;X-Generator: Poedit 3.0.1\\n\u0026#34; #: main.cpp:10 msgid \u0026#34;Hello World!\u0026#34; msgstr \u0026#34;Hallo Welt!\u0026#34; I already went ahead and translated Hello World! to Hallo Welt! using Poedit, but there are other tools like KBabel, Gtranslator, PO Mode and more which you can use to edit the .po files. The ecosystem is very mature and platforms like transifex also support it.\nNow with your fully or partially translated .po file in hand we will use msgfmt to create our final output file:\n1 msgfmt \u0026#34;locales/de/my-domain.po\u0026#34; --output-file=\u0026#34;locales/de/my-domain.mo\u0026#34; You now have a .mo file that can be loaded at runtime. But what happens when you change your code? How do you update your .pot, .po and .mo files when there are new, removed or changed strings in your code?\nSource changed, what to do? What we looked at so far is the initial setup phase. This is what you do when you have no previous .pot or .po files and generate them for the first time. If you already have them and the code changed you need to run xgettext again:\n1 xgettext main.cpp --keyword=\u0026#34;_\u0026#34; --output=\u0026#34;locales/my-domain.pot\u0026#34; The .pot file can be overwritten as you please since it only holds generated content. The .po files are more important since you don\u0026rsquo;t want to re-do all translations. For this reason we use msgmerge to update the .po files with the new template:\n1 msgmerge \u0026#34;locales/de/my-domain.po\u0026#34; \u0026#34;locales/my-domain.pot\u0026#34; --output-file=\u0026#34;locales/de/my-domain.po\u0026#34; The tool takes the current .po file and the new .pot file as input and spits out an updated .po file that keeps your existing translations as long as they are still used.\nAutomating and integrating with CMake If you found all of this extremely tedious to do by hand then you are not alone. Of course we can automate the generating and updating for all the required files as well as copying the .mo files to our output at build time using a CMake script.\nTo get started, grab a copy this script or add the repository as a submodule. In your CMakeLists.txt file we need to add some lines:\n1 2 3 4 5 6 7 8 9 10 # setup gettext set(GETTEXT_DOMAIN \u0026#34;my-domain\u0026#34;) set(GETTEXT_TARGET \u0026#34;gettext-target\u0026#34;) set(GETTEXT_OUTPUT_DIR \u0026#34;locales\u0026#34;) set(GETTEXT_LANGUAGES \u0026#34;en\u0026#34; \u0026#34;de\u0026#34;) target_compile_definitions(${PROJECT_NAME} PUBLIC \u0026#34;GETTEXT_DOMAIN=\\\u0026#34;${GETTEXT_DOMAIN}\\\u0026#34;\u0026#34;) target_compile_definitions(${PROJECT_NAME} PUBLIC \u0026#34;GETTEXT_OUTPUT_DIR=\\\u0026#34;${GETTEXT_OUTPUT_DIR}\\\u0026#34;\u0026#34;) First we specify some variables and add GETTEXT_DOMAIN and GETTEXT_OUTPUT_DIR as a predefined macro so we can use it in our code. I suggest you change the domain name and the list of languages you want to use.\nNext we want to add the previously downloaded script:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 include(\u0026#34;extern/gettext-cmake/Gettext_helpers.cmake\u0026#34;) CONFIGURE_GETTEXT( DOMAIN ${GETTEXT_DOMAIN} TARGET_NAME ${GETTEXT_TARGET} SOURCES \u0026#34;main.cpp\u0026#34; POTFILE_DESTINATION ${GETTEXT_OUTPUT_DIR} XGETTEXT_ARGS \u0026#34;--keyword=_\u0026#34; \u0026#34;--add-comments=TRANSLATORS:\u0026#34; \u0026#34;--package-name=${PROJECT_NAME}\u0026#34; \u0026#34;--package-version=${PROJECT_VERSION}\u0026#34; \u0026#34;--msgid-bugs-address=\u0026lt;https://github.com/erri120/${PROJECT_NAME}/issues\u0026gt;\u0026#34; \u0026#34;--copyright-holder=erri120\u0026#34; LANGUAGES ${GETTEXT_LANGUAGES} BUILD_DESTINATION $\u0026lt;TARGET_FILE_DIR:${PROJECT_NAME}\u0026gt;/${GETTEXT_OUTPUT_DIR} ALL ) There is lots to configure so let\u0026rsquo;s go through it all:\nDOMAIN set the domain name TARGET_NAME: set the CMake target name SOURCES: set a list of all source files that xgettext should look through, I suggest creating some variable that holds all your sources POTFILE_DESTINATION: this is the directory where the .pot file goes XGETTEXT_ARGS: most of the extra arguments supplied are just for flavor like specifying the package name, version as well as the address where you can report bugs and the copyright holder LANGAUGES: set the list of languages we want to support BUILD_DESTINATION: the top-level output folder for the generated .mo files ALL: adds the custom CMake target to the default build target so that it will be run every time And that\u0026rsquo;s it. Doing a CMake configure will generate the .pot and .po files, and you now have some new targets that will generate the .mo files.\nComplete Code Example 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 #include \u0026lt;iostream\u0026gt; #include \u0026lt;cstdlib\u0026gt; #include \u0026lt;libintl.h\u0026gt; #if WIN32 #define WIN32_LEAN_AND_MEAN #include \u0026lt;Windows.h\u0026gt; #endif #define _(STRING) gettext(STRING) static void setup_i18n(const std::string_view locale) { #if WIN32 // LocaleNameToLCID requires a LPCWSTR so we need to convert from char to wchar_t const auto wStringSize = MultiByteToWideChar(CP_UTF8, 0, locale.data(), static_cast\u0026lt;int\u0026gt;(locale.length()), nullptr, 0); std::wstring localeName; localeName.reserve(wStringSize); MultiByteToWideChar(CP_UTF8, 0, locale.data(), static_cast\u0026lt;int\u0026gt;(locale.length()), localeName.data(), wStringSize); _configthreadlocale(_DISABLE_PER_THREAD_LOCALE); const auto localeId = LocaleNameToLCID(localeName.c_str(), LOCALE_ALLOW_NEUTRAL_NAMES); SetThreadLocale(localeId); #else setlocale(LC_MESSAGES, locale.data()); #endif bindtextdomain(GETTEXT_DOMAIN, GETTEXT_OUTPUT_DIR); bind_textdomain_codeset(GETTEXT_DOMAIN, \u0026#34;UTF-8\u0026#34;); textdomain(GETTEXT_DOMAIN); } int main() { setup_i18n(\u0026#34;de\u0026#34;); std::cout \u0026lt;\u0026lt; _(\u0026#34;Hello World!\u0026#34;) \u0026lt;\u0026lt; std::endl; return EXIT_SUCCESS; } Woah, what happened with our 12 lines of code? Things are sadly not as easy as you want them to be. The main function is still the same, we have our marked string Hello World! but now there is the new function setup_i18n.\nThis new function takes a std::string_view as an argument and sets the current locale to something new. In this case I want to change the locale to de so my German translation for Hello World! can be loaded. In your actual code you\u0026rsquo;d want to have some language option the user can change which would call this function with the new locale.\nThe thing that makes this messy is of course the difference between systems. On a POSIX compliant system you can just call setlocale(LC_MESSAGES, \u0026quot;de\u0026quot;), but this doesn\u0026rsquo;t work at all for Windows.\nOn Windows you need to use SetThreadLocale which requires a locale ID that you can get from a name like de using LocaleNameToLCID. The _configthreadlocale(_DISABLE_PER_THREAD_LOCALE) call is important because SetThreadLocale only affects the current thread, who would have thought, so we want to disable this behavior and change the locale of this and all future threads.\nThe code after that uses the new predefined macros we added in our CMake file and I also added a call to the very useful bind_textdomain_codeset function. I suggest reading my Character encodings and Unicode where I explain the mess that is Code Pages and Unicode. With this function call gettext will always return a UTF-8 string. If you don\u0026rsquo;t want that or don\u0026rsquo;t need that you can remove this call but for frameworks like GTK it is required as they accept UTF-8 only.\nAlternatives GNU gettext has been around for over 30 years, it is battle tested, has great support and a matured ecosystem. However, if this is not for you or if you can\u0026rsquo;t use it there are a few alternatives available:\nDIY: always possible but highly discouraged Qt: of course the Qt-ecosystem has a different way of doing translations ICU: International Components for Unicode have ICU4C however it is not easy to use at all Boost.Locale they use ICU as a backend POSIX: catgets this was created back in 1987 before GNU gettext Win32: LoadString with this you can load string resources by ID ","permalink":"https://erri120.github.io/posts/2022-05-05/","summary":"GNU gettext is a very useful i18n and l10n tool that makes it easy to add translations in our code.","title":"Getting started with GNU gettext for C++"},{"content":"Time and Character encoding are the two things a programmer never wants to touch. Thankfully dealing with time has been made very easy, we developed ISO Standards, created standard libraries with good time functions and don\u0026rsquo;t have to worry until 2038 when a signed 32-bit integer is unable to hold the number of seconds elapsed since the Unix epoch.\nCharacter encodings did not receive this kind of treatment until the emergence of Unicode but even then we still have massive issues when dealing with them.\nHistory If you don\u0026rsquo;t want to read through this history class I prepared, you can skip directly to the more interesting topic here: Encoding, Code Pages and Unicode for Programmers.\nElectrical Telegraphy One of the earliest encoding methods is Morse Code which was introduced in the 1840s. The earliest code used commercially was the Cooke and Wheatstone telegraph five needle code, aka C\u0026amp;W5, but no one really used it. Instead, each country went ahead and developed their own code leading to the creation of the American Morse Code:\n1911 Chart of the Standard American Morse Characters from the American School of Correspondence\nThis code had issues so a fellow German named Friedrich Clemens Gerke developed a modified version in 1848 for use on German railways. At that time many central European countries belonged to the German-Austrian Telegraph Union and they quickly decided to adopt this version across all its countries in 1851.\nDue to the widespread use of the Gerke Code it became the International Morse Code in 1865:\nChart of the Morse code letters and numerals by Rhey T. Snodgrass \u0026amp; Victor F. Camp, 1922\nEven though it is called the \u0026ldquo;International\u0026rdquo; Morse Code, US companies refused to adopt it and continued to use the American Morse Code. They didn\u0026rsquo;t want to re-train their operators and because the telegraph was not state controlled but multiple private companies worked together, they never adopted it.\nSo why am I telling you this 180-year-old story? Back then we already were unable to come to a consensus on what standard to use. The Gerke Code was adopted by the German-Austrian Telegraph Union, but each country developed their own Code at some point because they used special characters in their language. This has been the biggest problem throughout the years. In Europe, we mostly use Latin-based alphabets but over in Asia things looked different:\nObsolete Chinese telegraph codes from Septime Auguste Viguier’s New Book for the Telegraph\nThis is one page from the Chinese Telegraph Code book. There are nearly 10 thousand characters in this book.\nThis is another recurring theme across history. Languages use different alphabets or none at all. A Logography uses written characters that represent a word or morpheme, like Chinese characters. This makes creating encodings for use everywhere really hard because now you don\u0026rsquo;t have 26 letters in lower and uppercase and a few syntax characters, but thousands of characters that have their own meaning. Most of the technological advancements in telegraphy and digital computers happened in Europe or USA, Asia was often left out and new encodings would focus on Latin-based alphabets.\nAutomatic Telegraphy In 1846 someone had the genius idea of automatically generating Morse code. Previously if you want to transmit a message, you\u0026rsquo;d go through each letter, look at the Morse Code table and press the required taps.\nBut what if you had a machine with multiple keys where each key corresponds to a different entry in the Morse Code table? A machine with multiple keys where each input corresponds to a different output, where have I heard that before? How about a piano:\nHughes Letter-Printing Telegraph Set built by Siemens and Halske in Saint Petersburg, Russia, ca.1900\nPiano keyboards existed for a long time and are really easy to understand. If you want to transmit an A you just press the key that is marked with an A. No need to look into some weird table and get hand pain by pressing the same key in different intervals for the entire day.\nBut let us not get side-tracked by random history and focus on out main topic: encoding. With these new printing telegraphs the operator stopped sending dots and dashes directly with a single key but instead operated a piano keyboard and a machine which would generated the appropriate Morse Code Point based on the key pressed.\nThe Morse Code was designed to be used by humans meaning common letters were easier to \u0026ldquo;type\u0026rdquo; by requiring fewer inputs. In the 1870s Émile Baudot created a new Code to be used by machines instead of humans to make sending and receiving even easier:\nPart of the patent from 1888 (US388244)\nThe Baudot Code is a 5-bit fixed-length binary code and next most important invention after the Morse Code. It is also known as the International Telegraph Alphabet No. 1 (ITA1).\nIf there is a No. 1, there must be a No. 2, so in 1901 Donal Murray modified Baudot Code to create the Murray Code. This code was used with punched paper tape. Now a reperforator could be used to make a perforated copy of received messages and a tape transmitter can send messages from punched tapes. Instead of directly transmitting to the line, the key presses of the operator would punch holes instead, making transmitting multiple messages from one tape very fast.\nOperator fatigue was no longer an issue, instead Murray focused on minimizing machine wear and had to add control characters to control the machine. These characters are Carriage Return and Line Feed also known as CR and LF. If you every wondered where those came from, now you know.\nIn 1924 the International Telegraph Union created the International Telegraph Alphabet No. 2 (ITA2), based on Murray Code, which became the most widespread code as nearly all 20th-century teleprinter equipment used ITA2 or some variant of it.\n1960s ITA2 was very successful but we were going digital. Here are some inventions from this era to paint a picture: IBM created the IBM 704 in 1954 which was the first mass-produced computer with floating-point arithmetic hardware, the first transatlantic communications cable was laid down in 1956 and MIT and Bell Labs created the first Modem in 1959.\nThings started to become digital and electronics became more important. Teleprinter technology also advanced and people wanted to use lowercase characters and additional punctuations. In 1964 the American Standards Association created the famous 7-bit ASCII Code also known as US-ASCII:\nIBM and Code Pages IBM with their mainframe computers played a very important role for us. They were a chief proponent of the ASCII standardization committee however they did not have enough time to prepare ASCII peripherals to ship with the IBM System/360 in 1964. This was a big problem and the company instead created the Extended Binary Coded Decimal Interchange Code (EBCDIC) which is an 8-bit character set.\nThe IBM System/360 was extremely successful and EBCDIC shared in this success. This was a problem, you now have ASCII which IBM really liked and EBCDIC which was used everywhere because everyone used the IBM System/360. Further complications arose since EBCDIC and ASCII were not compatible with each other which resulted in issues when transferring data between systems.\nWith EBCDIC came these new things called Code Pages. Not everyone speaks English and as we have seen before, some languages use a Latin-based alphabet, some use a non-Latin-based alphabet some don\u0026rsquo;t use an alphabet at all but Logography instead. Not only that but we are currently in the late 20th century when 20-megabyte drives costs 250 USD meaning we have to be space efficient.\nFor these reasons, IBM created code pages for the EBCDIC character set which are represented by a number and change the way you encode certain characters. One important thing I want to mention is that IBM created the code pages but not the standard that was behind it. As an example let\u0026rsquo;s look at JIS X 0201 which is a Japanese Industrial Standard developed in 1969 and was implemented by IBM as Code Page 897. IBM did not create the standard, they only created the code page that implemented it.\n8-bit architecture In the 1980s the 8-bit architecture led to the 8-bit byte becoming the standard unit of computer storage, so ASCII with its 7-bit length was inconvenient for data retrieval. Thus, in 1987 we got the standard ISO 8859-1 aka Extended ASCII which uses the extra bit for more non-English characters like accented vowels and some currency symbols.\nGoing Unicode It is the year 1980 and a company named Xerox created the Xerox Character Code Standard (XCCS) which is 16-bit and encodes the characters required for languages using the Latin, Arabic, Hebrew, Greek and Cyrillic scripts, the Chinese, Japanese and Korean writing systems, and technical symbols.\nA group with members of Xerox and Apple started thinking about a universal character set in 1987 and used the XCCS as an inspiration. This group quickly grew as people from Sun Microsystems, Microsoft and other companies started to join.\nThe Unicode Consortium which was incorporated in early 1991 published the first volume of the Unicode Standard later that year and the second volume in the next year to include a total of 28,327 characters.\nEncoding, Code Pages and Unicode for Programmers Now that the history class is over we can look at some code.\nWindows API If you ever wrote some C/C++ code and had to work with the Windows API you might wonder why there are multiple versions of the same function like MessageBox, MessageBoxA and MessageBoxW:\n1 2 3 4 5 6 7 8 int MessageBoxA(HWND hWnd, LPCSTR lpText, LPCSTR lpCaption, UINT uType); int MessageBoxW(HWND hWnd, LPCWSTR lpText, LPCWSTR lpCaption, UINT uType); #ifdef UNICODE #define MessageBox MessageBoxW #else #define MessageBox MessageBoxA #endif The docs say A means ANSI and the W stands for Unicode, but this is a bit misleading so here is an explanation.\nFirst ANSI is just straight up confusing and a \u0026ldquo;misnomer\u0026rdquo;.\nA misnomer is a name that is incorrectly or unsuitably applied.\nMicrosoft themselves said it\u0026rsquo;s stupid:\nANSI: Acronym for the American National Standards Institute. The term “ANSI” as used to signify Windows code pages is a historical reference, but is nowadays a misnomer that continues to persist in the Windows community. The source of this comes from the fact that the Windows code page 1252 was originally based on an ANSI draft—which became International Organization for Standardization (ISO) Standard 8859-1. “ANSI applications” are usually a reference to non-Unicode or code page–based applications.\nSo going forward I\u0026rsquo;m just going to call it \u0026ldquo;Windows Code Pages\u0026rdquo;.\nNext up is the W for Unicode. The W comes from wchar_t which is an implementation-defined wide character type. In the Microsoft compiler, it represents a 16-bit wide character used to store Unicode encoded as UTF-16LE.\nSo let\u0026rsquo;s recap:\nMessageBoxA: accepts the 8-bit char type and uses the Windows Code Pages MessageBoxW: accepts the implementation-defined wide wchar_t type and uses UTF-16 MessageBox: just an alias that will use either MessageBoxA or MessageBoxW The fact that wchar_t is implementation-defined is obviously a problem. Windows adopted Unicode when it fit in a 16-bit long type, but that is not the case anymore.\nNow for some code and some experiments:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 #define WIN32_LEAN_AND_MEAN #include \u0026lt;iostream\u0026gt; #include \u0026#34;Windows.h\u0026#34; int main() { MessageBoxA(nullptr, \u0026#34;This uses Windows Code Pages\u0026#34;, nullptr, MB_OK); MessageBoxW(nullptr, L\u0026#34;This uses UTF-16\u0026#34;, nullptr, MB_OK); constexpr char helloShiftJis[] = {static_cast\u0026lt;char\u0026gt;(0x82), static_cast\u0026lt;char\u0026gt;(0xB1), static_cast\u0026lt;char\u0026gt;(0x82), static_cast\u0026lt;char\u0026gt;(0xF1), static_cast\u0026lt;char\u0026gt;(0x82), static_cast\u0026lt;char\u0026gt;(0xC9), static_cast\u0026lt;char\u0026gt;(0x82), static_cast\u0026lt;char\u0026gt;(0xBF), static_cast\u0026lt;char\u0026gt;(0x82), static_cast\u0026lt;char\u0026gt;(0xCD), \u0026#39;\\0\u0026#39;}; MessageBoxA(nullptr, helloShiftJis, nullptr, MB_OK); MessageBoxW(nullptr, L\u0026#34;こんにちは\u0026#34;, nullptr, MB_OK); return 0; } The first two calls show what you\u0026rsquo;d expect to see as they only contain Latin characters. The other calls are more interesting. This hideous array initialization contains the bytes of \u0026ldquo;こんにちは\u0026rdquo; encoded in Shift-JIS. My system locale is set to \u0026ldquo;English (United States)\u0026rdquo; which means my Windows uses Code Page 1252 aka windows-1252. This Code Page does not contain any of the Hiragana characters and instead of seeing \u0026ldquo;こんにちは\u0026rdquo; on screen I get \u0026ldquo;‚±‚ñ‚É‚¿‚Í\u0026rdquo;. If change my system locale to \u0026ldquo;Japanese (Japan)\u0026rdquo; then Windows would use Shift-JIS aka windows-932 and display \u0026ldquo;こんにちは\u0026rdquo; correctly. The MessageBoxW call with L\u0026quot;こんにちは\u0026quot; correctly displays \u0026ldquo;こんにちは\u0026rdquo; because it\u0026rsquo;s UTF-16 encoded.\nThe Windows API also provides functions for converting between string types:\n1 2 3 4 5 // convert from a Code Page to UTF-16 int MultiByteToWideChar(UINT CodePage, dwFlags, LPCCH lpMultiByteStr, int cbMultiByte, LPWSTR lpWideCharStr, int cchWideChar); // convert from UTF-16 to a Code Page int WideCharToMultiByte(UINT CodePage, DWORD dwFlags, LPCWCH lpWideCharStr, int cchWideChar, LPSTR lpMultiByteStr, int cbMultiByte, LPCCH lpDefaultChar, LPBOOL lpUsedDefaultChar); With the first function we can convert our Shift-JIS encoded string into UTF-16 and correctly display it:\n1 2 3 4 5 6 auto bufferSize = MultiByteToWideChar(932, MB_ERR_INVALID_CHARS, helloShiftJis, -1, nullptr, 0); auto converted = new wchar_t[bufferSize]; MultiByteToWideChar(932, MB_ERR_INVALID_CHARS, helloShiftJis, -1, converted, bufferSize); MessageBoxW(nullptr, converted, nullptr, MB_OK); Current Issues We have looked at the history of character encodings and some examples with the Windows API. Now it\u0026rsquo;s time to take a look the issues we still have.\nThe Web is united under UTF-8 with over 98% of all web pages using it. This is further enforced by standards like JSON which require UTF-8 encoding. As good as this is, the desktops are still far behind UTF-8 adoption.\nWindows started supporting UTF-8 with Windows XP but only since Windows 10 version 1903 is it the default character encoding for Notepad. These editors are the main culprits as they often default to the current Windows Code Page which makes sharing files internationally a pain.\nThankfully everything is starting to or already using UTF-8, newer languages like Go and Rust basically force you to use UTF-8 and even Microsoft said you should start using it.\nWindows Code Pages are legacy but because it\u0026rsquo;s still used in production we continue to have issues with character encoding. If you have some issues I recommend trying Locale Emulator.\nClosing Words Props to you if you read this entire thing. I personally had to deal a lot with encoding issues as a lot of games I play come from Japan and don\u0026rsquo;t work on my machine without a locale emulator.\nI hope this answered some questions you might have around this topic. It is very complex and has a very long history, but this should give you a peak into the issues we still have.\n","permalink":"https://erri120.github.io/posts/2022-04-15/","summary":"An in-depth post detailing the origins of Unicode, Code Pages and Character Encodings.","title":"Everything about Unicode, Code Pages and Character Encoding"},{"content":"Edit (Sep 2022): Update 3.0 has made this post obsolete. This post was written before 3.0 and the proposed solution only works with earlier versions. This post remains available for the sake of preservation and because it\u0026rsquo;s funny reading the ramblings of an angry person.\nI love Anno 2070. The game released in 2011, and I bought the Königsedition, a special deluxe edition containing the base game, the Deep Ocean DLC, a poster and the entire soundtrack on 3 CDs, back in 2013. I played this game to death, it is my most played strategy game and I have often look back at the fun I had while playing. If you never played the game and went on Steam you might notice it has a Mixed rating. This might seem weird considering I just told you how awesome this game is. But the ratings reflect the current state very well: you can\u0026rsquo;t fucking play the game.\nWhy the game is unplayable Did you think Cyberpunk 2077 at launch was bad with all the glitches and bugs? At least you could play that game and somewhat enjoy it. Now look at this 10-year-old game that you can\u0026rsquo;t even fucking play and Ubisoft completely abandoned it. A studio abandoning their games is nothing new and often not a problem. It might suck that the multiplayer servers of your favorite game go offline, but you could at least continue playing the campaign or the community might start hosting servers.\nSo what do I mean with \u0026ldquo;you can\u0026rsquo;t fucking play the game\u0026rdquo;? Let\u0026rsquo;s imagine an obstacle course where the finish is the main menu of the game. To get to the main menu you first have to start the game either through Uplay, Steam or by starting the Anno5.exe directly. The first obstacle is Uplay itself. I can not count the number of times Uplay had some error or bug that lead to me not playing a game. Back when Assassin\u0026rsquo;s Creed Unity came out, Uplay thought my saves were ephemeral, and I had to replay the first 5 hours of the game more than 13 times. The launcher is a complete dumpster fire and worse than the League of Legends client.\nIf you start Anno 2070 then Uplay will also launch and probably kill the process and start the Auto Updater. If that happens you are fucked because now the \u0026ldquo;Auto Updater\u0026rdquo; will try to auto update your game, even though it\u0026rsquo;s already updated. Once it\u0026rsquo;s done updating it will start the game which will start Uplay which will probably start the updater again. If you bought the game on Steam then you have to flip a coin and if it lands on heads you are fucked and Uplay doesn\u0026rsquo;t think you own the game. If you somehow did manage to convince Uplay to work for once you are now faced with the login screen.\nThe login screen: You open a website, click the login button, enter your username + password and log in to your account. But why do you have to log in to your account in Anno 2070? Some time around 2013 Ubisoft had the genius idea of switching up their entire account system. Anno 2070 was caught up in the transition and somehow came out as an abomination where the login is more broken than Fallout 76 on release. So what username + password do you use in Anno 2070? If you think you have to use your Uplay account then you\u0026rsquo;re wrong. If you choose your Steam credentials then you are also wrong. You have to use the credentials of the account you used to register the game. In my case it\u0026rsquo;s not my main Uplay account because it didn\u0026rsquo;t even exist back then. Now here is the fun part: I still own the game on Uplay. It is in my library, but I can\u0026rsquo;t use my account because the key is not linked to my actual Uplay account. I put emphasis on key because that is our next hurdle. Even if you manage to log in you will probably be asked about the key however your key might be linked to another account even though that account doesn\u0026rsquo;t have the game in their Uplay library. I don\u0026rsquo;t even know how this exactly works with Steam users, but I can imagine it doesn\u0026rsquo;t.\nI hope you come to understand how utterly broken the start procedure of Anno 2070 is. Back in the day when Ubisoft used something called SolidShield the process was even more fucked. Your key was not only tied to some random account but also to your hardware directly meaning if you upgraded your GTX 580 to a GTX 590 your key would not work. If you somehow think this is just the rambling of an insane person then you are correct, but I would also like you to check out the Steam reviews and this great post on reddit \u0026ldquo;Ubisoft\u0026rsquo;s Anno 2070 Unplayable Due to DRM\u0026rdquo; for more salt.\nFixing the game Edit (Dec 2021): Before we dive into this I want to tell you that there are other potential fixes to this problem. Some people messaged me and asks why I didn\u0026rsquo;t do X or Y and my answer was always \u0026ldquo;It didn\u0026rsquo;t work\u0026rdquo;. If you have a similar problem with Anno 2070 then there are other solutions you can try. The fix I will propose is guaranteed to work but should be a last resort.\nNow that I cooled off and finished my rant it\u0026rsquo;s time to talk about how we can fix everything. If you want to follow along you need to get the latest version from Uplay or Steam. It has the version number 2.0.7792.0 and the SHA256 hash of Anno5.exe is C76D42E71AF6A7D1786C6846091A5FBDDB13E3A88A72E469F36F9F365645D58A. If you don\u0026rsquo;t want an explanation of how I got here you can skip to the TL;DR but if you want to hear the explanation then knowing assembly and basic programming is recommended.\nThe goal is to force the game into offline mode. Simply changing your firewall settings to block all connections from the executable is not the solution because, as explained earlier, we also need to deal with Uplay. There is actually a single function responsible for launching Uplay. The standard solution would be to either replace the function with NOP instructions or replace the CALL instruction with NOP. This would probably work, however I found something better. This InitializeUplay function gets called only once: during a pre-init function of the game inside a condition. There is actually a JNZ instruction which will skip the function call entirely.\nThe comparison at 004b05bf (80 7e 50 00 bb 10 00 00 00) compares whatever is at ESI+50 with 0 meaning that the JNZ jump at 004b05c8 will be taken if ESI+50 is not 0. This is never the case and I don\u0026rsquo;t really understand what the condition actually represents, but we can exploit this and make sure the jump is always taken. Since ESI+50 is always 0 we can just change the comparison to compare ESI+50 with 1: 80 7e 50 01. With this you can launch the game and Uplay will never start.\nNext up is the login screen. This took me the longest to figure out. I found the Uplay initialization within a few hours, but this took me an entire day. We can\u0026rsquo;t really skip the login screen, trust me I tried that, but instead we just have to trick the game that we are offline and want to play in offline mode. I used to play a lot in offline and back in the day you still had to put your username + password and click login at which point the game will see you are offline and then loads your offline profile. So now it\u0026rsquo;s time to find the callback function of the login button. I did manage to find the callback function and noticed a little function that returns a WebAdapter based on the argument. Let\u0026rsquo;s name this function CreateWebAdapter(int x) with x = 0 returning an offline web adapter and x = 1 the normal one.\nMy spidey senses were tingling, and I knew this was related to the solution. The login button callback function however will always call CreateWebAdapter(1). You could change the 1 to a 0, but I noticed something different. You see, the login frame is created programmatically, and you can clearly see where the username text box, password text box, remember user ID checkbox, register account button and login button get added to the frame. In order to figure out which button has what callback I put a breakpoint on every callback function and clicked every button. However, there was 1 unused button. I\u0026rsquo;m not sure if it\u0026rsquo;s unused or only conditionally used but guess what function the callback calls: CreateWebAdapter(0). This possibly unused callback function was very similar to the login button callback, but it will always use the offline web adapter. I was very excited when I found out about this and immediately changed the callback function of the login button to use the callback function of the unused button by changing the PUSH instruction at 0068a57b from 68 90 9c 68 00 to 68 d0 8f 68 00, and it fucking worked. You can now log in and start in offline mode.\nAnno 2070 Main Menu\nTL;DR if you don\u0026rsquo;t want an explanation Change the CMP instruction at 004b05bf (80 7e 50 00 bb 10 00 00 00) from 80 7e 50 00 to 80 7e 50 01 Change the PUSH instruction at 0068a57b from 68 90 9c 68 00 to 68 d0 8f 68 00 If you did everything right the SHA256 hash of the modified Anno5.exe file should be 01E123A72C3DCB4FB1E018A685692F57B9586FF4F187BB23B09D8B93D754C268. If that is not the case you did something wrong and your game is probably broken so load your backup and try again.\nAnno 2070 setting tips In case you want to play Anno 2070 after reading this post here are some changes you might want to make in your settings file. The file is located in %appdata%/Ubisoft/Anno 2070/Config/Engine.ini\nchange language by editing \u0026lt;LanguageTAG\u0026gt;ger\u0026lt;/LanguageTAG\u0026gt; skip the intro with \u0026lt;SkipIntro\u0026gt;1\u0026lt;/SkipIntro\u0026gt; Afterword This project has been a lot of fun and ended rather suddenly. I initially allotted 3 full days to this endeavor, but I kept finding new leads and getting new ideas, so I ended up only spending around 12.5 hours (I love how x32dbg will tell you how much time you have wasted debugging). The solution is also very simple, and I made it the title of this post. Just 3 bytes to fix the entire start of the game. I thought I would have to modify a lot of functions or write some DLL I\u0026rsquo;d have to inject but no, you just change 3 bytes and be happy :)\nI hope you found this post somewhat interesting, and maybe you can finally play Anno 2070. If this works for you please do tell me below.\n","permalink":"https://erri120.github.io/posts/2021-12-10/","summary":"Anno 2070 has been unplayable for multiple years but you can still buy it. This post will explain how you can fix the game by changing 3 bytes.","title":"Fixing Anno 2070 by changing three bytes"},{"content":"rpgmpacker has been one of my biggest non-modding related projects on GitHub and the only reason people visit my blog:\nTop queries of my blog\nThe developer of the game Star Knightess Aura (NSFW) is the first actual user of this tool and contributed a good amount of issues on GitHub driving the development of this project forward.\nWith some issues still open I stopped working on this project in April 2021 because I knew I had to rework this tool at some point and didn\u0026rsquo;t have the motivation to do so at that time. I used C++ for 1.x because I wanted to finally build something with this language. I had only done small assignments in Uni with C++ but never done anything more with it, mainly because the language sucks ass.\nC/C++ is old, very old, and it clearly shows, not just in the language but the tooling available. Ask 20 C++ developers what build system they use, and you will likely get 10 different answers. Working with dependencies is a nightmare when it comes to C/C++ development. It\u0026rsquo;s actually impossible to count the amount of times I wanted to contribute to a C++ project only to be blocked by some weird build system that only works in a specific environment when the stars align and the moon is in the Waxing Gibbous lunar phase. Once you did get something to work you now have to deal with the language itself. Let me ask you this: how do you represent a string in C++? Did you know that there is std::string which is actually just std::basic_string\u0026lt;char\u0026gt;, std::u16string which is std::basic_string\u0026lt;char16_t\u0026gt;, std::u32string which is std::basic_string\u0026lt;char32_t\u0026gt; and std::wstring which is std::basic_string\u0026lt;wchar_t\u0026gt; but don\u0026rsquo;t let that fool you because there is also char8_t from C++ 20 that introduces std::u8string and don\u0026rsquo;t forget that wchar_t has different sizes depending on the platform, compiler and time of day?\nC++ is a fucking mess. Working with strings in C++ should not be this hard.\nSo lets calm down and look at something different: Rust. I love Rust, it\u0026rsquo;s C++ in way better and more modern. I wanted to rework rpgmpacker in Rust and actually did exactly that for quite some time but soon realized this was also not the way.\nRPG Maker MV/MZ is pure JavaScript with the data files being in JSON. The developers are probably familiar with JavaScript as well so considering the ecosystem, it is more fitting to create a JavaScript tool. Of course, I wouldn\u0026rsquo;t be doing that, fuck vanilla JavaScript, I\u0026rsquo;m using TypeScript.\nI like to use the right tool for the job. Even though I\u0026rsquo;m quite the C# and Rust fanatic I went with TypeScript because it makes life very easy. I don\u0026rsquo;t have to write complex JSON parsers or DTOs, I can just do JSON.parse(...) and access whatever I want. One huge problem was also parsing the plugins for the exclude-unused feature. If you open js/plugins.js in your game, you will find a variable $plugins. With TypeScript I can just use the VM to dynamically load this file and access this variable directly. With any other language I would have to read the file as a string and do some complex string parsing.\nOf course there are some problems with TypeScript. I previously complained about the C++ dependency systems but the huge dependency trees you can get when using NPM are also insane. Before I even started on the rework I set my goal to require the least amount of modules which ended up being only 1: yargs because parsing arguments is not something I want to implement myself.\nI hope the rework will be received pleasantly by the 2 people using the tool.\n","permalink":"https://erri120.github.io/posts/2021-11-18/","summary":"A quick post explaining my decision of moving from C++ to TypeScript for my project rpgmpacker.","title":"Reworking rpgmpacker: Moving from C++ to TypeScript"},{"content":"Time and Frames We have all heard the term FPS, frames per second, which tells us how many frames are rendered on screen within one second. Higher FPS will result in smoother animations because those animations get more frames. The problem with high FPS is that the GPU has to keep up with this demand and be able to produce 30/60/144 or more frames within 1 second. This is where frame times, VSync and a bunch of offer stuff that I won\u0026rsquo;t cover here comes into play.\nThe important takeaway is that you can not reliably calculate a duration based on how many frames where rendered because not every frame takes the same amount of time. This is a problem when you want to smoothly move an object from one position to another. Engines like Unity solve this by providing a delta time which is the interval from the last to the current frame, allowing you to create smooth translations.\nLooking at RPG Maker MV/MZ So what issue does RPG Maker have? I have recently played a game for around 21 hours, but the save menu displayed over 51 hours, more than double. Actually not just more than double but around 2.4 times more than expected. This quickly made me realize that I had been playing the game on my 144Hz monitor, meaning instead of rendering at 60 FPS, the game was rendering at 144 FPS. I hope you notice that 60 x 2.4 equals 144.\nTo further investigate the problem I took a peek inside the js folder, did a quick search for \u0026ldquo;playtime\u0026rdquo; and found the culprits of my frustration:\n1 2 3 4 Game_System.prototype.onBeforeSave = function() { this._framesOnSave = Graphics.frameCount; // ... }; 1 2 3 4 Game_System.prototype.onAfterLoad = function() { Graphics.frameCount = this._framesOnSave; // ... }; 1 2 3 Game_System.prototype.playtime = function() { return Math.floor(Graphics.frameCount / 60); }; 1 2 3 4 5 6 Game_System.prototype.playtimeText = function() { var hour = Math.floor(this.playtime() / 60 / 60); var min = Math.floor(this.playtime() / 60) % 60; var sec = this.playtime() % 60; return hour.padZero(2) + \u0026#39;:\u0026#39; + min.padZero(2) + \u0026#39;:\u0026#39; + sec.padZero(2); }; A quick explanation: whenever you save, the game includes the amount of frames it has rendered in the save file because Game_System is serialized and _framesOnSave is a field of that object. When you load, the save will get deserialized and Graphics.frameCount will be set back to that value. In case you are wondering: Graphics.frameCount gets incremented on each render.\nThis is fine so far, but the real problems are found in the playtime and playtimeText functions where the game assumes you are constantly playing at 60 frames per second. The pure frameCount value gets divided by 60 to get the amount of seconds passed and in playtimeText that result is further processed to the get hours and minutes.\nDue to the multiple issues outlined at the beginning you can see that this is not a good idea. Having said that: let\u0026rsquo;s look at how to fix this.\nPossible Solution This solution is probably the easiest and most straightforward one I can think of:\non load/start: set startTime to the current time on save: get the current time and calculate the difference between now and startTime add the difference to a variable in the save No need for frame count calculations or anything complex, just use Date.now() a few times and get the difference.\nChanging the implementation in RPG Maker MV/MZ Start by opening js/rpg_objects.js and look for the initialize function. Here we want to add two new fields:\n1 2 3 4 5 Game_System.prototype.initialize = function() { this._startTime = Date.now(); this._playtime = 0; // ... } Date.now() returns the current Unix time as an integer that looks like this: 1632928643900.\nWe need to set the start time in the initialize function for when the game starts and in the onAfterLoad function for when the player loads a save file:\n1 2 3 4 Game_System.prototype.onAfterLoad = function() { this._startTime = Date.now(); // ... } The onBeforeSave function gets called before the save is serialized so here we have to calculate the time difference and reset the start time:\n1 2 3 4 5 Game_System.prototype.onBeforeSave = function() { this._playtime += Date.now() - this._startTime; this._startTime = Date.now(); // ... } Assuming the player started at 1632928643900 and played for 1 second till 1632928644900, this._playtime will now be 1000. The only thing remaining is updating the playtime function:\n1 2 3 Game_System.prototype.playtime = function() { return Math.floor(this._playtime / 1000); }; Since this function returns the amount of time passed in seconds we have to divide by 1000 to convert from milliseconds to seconds. While we are at it, I also recommend updating playtimeText:\n1 2 3 4 5 6 7 8 9 10 11 Game_System.prototype.playtimeText = function() { var secondsPassed = this.playtime(); var minutesPassed = Math.floor(secondsPassed / 60); var hoursPassed = Math.floor(minutesPassed / 60); var sec = secondsPassed % 60; var min = minutesPassed % 60; var hour = hoursPassed % 24; return hour.padZero(2) + \u0026#39;:\u0026#39; + min.padZero(2) + \u0026#39;:\u0026#39; + sec.padZero(2); }; The main difference here is that we cache the result from the playtime function and other operations.\nPlugin for MV/MZ If you don\u0026rsquo;t want to mess with the game files directly, which is a good idea, then I suggest taking a look at the PlaytimeFix plugin which I developed that does everything explained in this post.\nAfterword This issue has been bugging me a lot during my playthroughs of longer games because you don\u0026rsquo;t really notice it when your playtime goes from 1h to 2 hours but going from 21 hours to 50+ hours was a bit more in my face. RPG Maker MZ also doesn\u0026rsquo;t fix this, and I have not seen any discussion about this.\n","permalink":"https://erri120.github.io/posts/2021-09-29/","summary":"RPG Maker MV/MZ uses the amount of rendered frames to calculate how long player has been playing the game. This is beyond stupid.","title":"Don't count frames to calculate time"},{"content":"Automation saves time and therefore money. However, you have to ask yourself if it\u0026rsquo;s worth spending hours writing a complex and long script. In the worst case you might end up wasting instead of saving time. Thankfully most programming related tasks can be automated very quickly and each language or framework often has documentation on how to get you up and running.\nThis is sadly not often the case when it comes to game development. Popular engines like Unreal Engine and Unity have automation tools1 and documentation for automating the build process but less popular ones such as RPG Maker don\u0026rsquo;t have those.\nIn this post I will explain what CI/CD means and how to set up a pipeline for RPG Maker MV/MZ with rpgmpacker. A big chunk of this post is explaining CI/CD and how RPG Maker MV/MZ works, if you just want to go to the practical part click here.\nCI/CD for non-programmers RPG Maker is very non-programmer friendly, and you can easily end up not having to write a single line of code when creating a game with it. For this reason I want to make sure we all have the same base level understanding of CI/CD before continuing. That being said, I expect you to know about version control with git so make sure to watch these two amazing videos by Fireship if you are unfamiliar with it:\nLet\u0026rsquo;s get started with Continuous Integration which means running a pipeline on every change you commit to a repository. This pipeline consists of one or multiple jobs where one job could be running some tests or building the project. If you run the tests on every change then you can easily track down the commit where a bug was introduced to the codebase. A common pipeline would look like this:\nsetup (checkout, getting all the tools) running tests building the project packaging uploading an artifact (fancy word for an archive containing the output) The terminology can be different depending on your provider for example a pipeline on GitHub is called a workflow and each workflow has steps instead of jobs. They often mean the same thing so don\u0026rsquo;t let that confuse you.\nContinuous Delivery is about delivering the artifact we produced previously. This artifact can be sent to testers, friends or the QA team. Continuous Deployment is also a thing and means actually deploying the artifact by uploading the next release of a game to itch.io or deploying the software to your servers.\nBig studios often end up with huge pipelines because they have many employees and teams. They might have a CI pipeline that runs the tests and builds the project. The artifact would then send in a CD pipeline to the testers or QA team and when they approve of the build it gets released into the world. This is probably not the case for you or your team and most of the time you can combine all of this into one pipeline.\nIf you are curious how such a pipeline might look like, here is the GitHub Actions workflow file that builds (CI) and deploys (CD) this site to GitHub Pages on every push to the master branch:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 on: push: branches: [ master ] jobs: build: runs-on: ubuntu-latest steps: # setup - uses: actions/checkout@v2 with: submodules: true fetch-depth: 0 - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; # building the site (CI) - name: Build run: hugo --minify # deploying the site (CD) - name: Deploy uses: peaceiris/actions-gh-pages@v3 with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public publish_branch: gh-pages RPG Maker MV/MZ manual export RPG Maker MV Deployment Window\nThis is the deployment window of RPG Maker MV which you can access in the editor under File -\u0026gt; Deployment. The major issue with this is the fact you have to use it at all. Software developers love the tools they can use from the command line because they can be easily accessed using a script.\nOther engines like Unreal Engine have to compile code before deployment, but RPG Maker is very different: It\u0026rsquo;s only copying files from one place to another.\nThere is no code to compile because all the code is in JavaScript, it doesn\u0026rsquo;t even need to compile an executable because it just uses NW.js. RPG Maker is one of the simplest game engines, not just in terms of usability but how everything is just JSON and JavaScript. You can literally go into the js folder and look at all the source code of the engine. All the data is saved as JSON so if you want to change the name of an item you can open data/Items.json and change the values.\nIf you are curious where all those files come from, open your RPG Maker MV/MZ installation folder and look for folders that start with nw-js-. The files inside those folders are copied over to the output directory. You can also find the default template files in the NewData folders.\nSimply copying files from one place to another can be done with a script. This was exactly what rpgmpacker did in my first prototype, but I soon realized that there was more going on.\nFirst we have file filtering. For audio, you can have .ogg and .m4a files but exporting for Windows/OSX will only copy the .ogg files while exporting for Mobile will only copy .m4a files. Then we have image and audio \u0026ldquo;encryption\u0026rdquo; and the exclude-unused feature which requires parsing of almost all JSON files to figure out which files are actually in use and which are not. There is even more to this which can\u0026rsquo;t be done in a simple script, so I decided to build an entire tool which we are going to use in the pipeline.\nCreating a Release Pipeline for RPG Maker The pipeline we are going to build will do the following:\nrun rpgmpacker zip the output upload the output to itch.io using Butler If your game is not on itch.io then you can leave out the last step or replace it with an upload to MEGA, GDrive or your hosting platform of choice.\nFor the pipeline you can write a Bash or PowerShell script, just pick whatever you like more:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash INPUT=\u0026#34;./MyProject\u0026#34; OUTPUT=\u0026#34;./build\u0026#34; RPGMAKER=\u0026#34;M:\\\\SteamLibrary\\\\steamapps\\\\common\\\\RPG Maker MV\u0026#34; npx rpgmpacker@latest \\ --input \u0026#34;$INPUT\u0026#34; \\ --output \u0026#34;$OUTPUT\u0026#34; \\ --rpgmaker \u0026#34;$RPGMAKER\u0026#34; \\ --exclude \\ --noempty \\ --debug \\ --platforms \u0026#34;Windows\u0026#34; \u0026#34;OSX\u0026#34; \u0026#34;Linux\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 $PROJECT=\u0026#34;./MyProject\u0026#34; $OUTPUT=\u0026#34;./build\u0026#34; $RPGMAKER=\u0026#34;M:\\\\SteamLibrary\\\\steamapps\\\\common\\\\RPG Maker MV\u0026#34; npx rpgmpacker@latest ` --input $PROJECT ` --output $OUTPUT ` --rpgmaker $RPGMAKER ` --exclude ` --noempty ` --debug ` --platforms \u0026#34;Windows\u0026#34; \u0026#34;OSX\u0026#34; \u0026#34;Linux\u0026#34; In my case I have the project in a folder called MyProject next to the script and the output should go into the build folder. I\u0026rsquo;m using the Steam version of RPG Maker MV for this but any version of MV or MZ can be used.\nNext up we want to zip the output. rpgmpacker will create an output folder for each platform you specified so for me I will have build/Windows, build/OSX and a build/Linux folder.\n1 2 3 for platform in $OUTPUT/*; do 7z a -tzip -o$OUTPUT $platform.zip $platform/* done 1 2 3 4 $Platforms = Get-ChildItem -Path $OUTPUT -Directory -Name foreach ($platform in $Platforms) { 7z a -tzip -o$OUTPUT $OUTPUT/$platform.zip $OUTPUT/$platform/* } This is where the difference between PowerShell and Bash start to appear. In Bash, we use a glob to get all top-level items of the output directory and in PowerShell we use the Get-ChildItem function. They look different, but they do the same thing with the main difference being that platform in Bash will be ./build/Windows while $platform in PowerShell will be just Windows. This is why we need to prefix $platform with $OUTPUT/ when calling 7z.\nThe pipeline is almost done at this point. Next is publishing the output with Butler to itch.io but if you are not using that platform you are now done.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # see https://broth.itch.ovh/butler ButlerChannel=\u0026#34;windows-amd64\u0026#34; # darwin-amd64 for Mac and linux-amd64 for Linux ButlerVersion=\u0026#34;15.21.0\u0026#34; ButlerOutput=\u0026#34;butler-$ButlerVersion-$ButlerChannel\u0026#34; if [ ! -d $ButlerOutput ]; then curl -L -o butler.zip https://broth.itch.ovh/butler/$butlerChannel/$butlerVersion/archive/default unzip -o butler.zip -d $ButlerOutput rm butler.zip else echo \u0026#34;Butler version $ButlerVersion ($ButlerChannel) has already been downloaded\u0026#34; fi # change butler.exe to just butler if you are on Unix butler=\u0026#34;$ButlerOutput/butler.exe\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # see https://broth.itch.ovh/butler $ButlerChannel=\u0026#34;windows-amd64\u0026#34; # darwin-amd64 for Mac and linux-amd64 for Linux $ButlerVersion=\u0026#34;15.21.0\u0026#34; $ButlerOutput=\u0026#34;butler-$ButlerVersion-$ButlerChannel\u0026#34; if (![System.IO.Directory]::Exists($ButlerOutput)) { Invoke-WebRequest -OutFile butler.zip \u0026#34;https://broth.itch.ovh/butler/$butlerChannel/$butlerVersion/archive/default\u0026#34; Expand-Archive -Path butler.zip $ButlerOutput Remove-Item -Path butler.zip } else { Write-Output \u0026#34;Butler version $ButlerVersion ($ButlerChannel) has already been downloaded\u0026#34; } # change butler.exe to just butler if you are on Unix $butler=\u0026#34;$ButlerOutput/butler.exe\u0026#34; If you are not on Windows you should change the ButlerChannel variable to darwin-amd64 or linux-amd64 depending on your system. I recommend checking the channel list of Butler if you need something different. I\u0026rsquo;ve added a simple check, so we don\u0026rsquo;t download Butler whenever we call the script. Before we continue you should get an API key from itch.io. You should treat API keys like passwords and should never make them public. A good way of storing API keys is with environment variables and Butler expects the API key to be stored in the BUTLER_API_KEY environment variable.\n1 2 3 4 5 6 7 export BUTLER_API_KEY=\u0026#34;YOUR_KEY\u0026#34; ButlerProject=\u0026#34;YOUR_USERNAME/YOUR_PROJECT_NAME\u0026#34; ./$butler login ./$butler push $OUTPUT/Windows $ButlerProject:windows-beta ./$butler push $OUTPUT/OSX $ButlerProject:osx-beta ./$butler push $OUTPUT/Linux $ButlerProject:linux-beta 1 2 3 4 5 6 7 $env:BUTLER_API_KEY=\u0026#34;YOUR_KEY\u0026#34; $ButlerProject=\u0026#34;YOUR_USERNAME/YOUR_PROJECT_NAME\u0026#34; \u0026amp;$butler login \u0026amp;$butler push $OUTPUT/Windows.zip \u0026#34;${ButlerProject}:windows-beta\u0026#34; \u0026amp;$butler push $OUTPUT/OSX.zip \u0026#34;${ButlerProject}:osx-beta\u0026#34; \u0026amp;$butler push $OUTPUT/Linux.zip \u0026#34;${ButlerProject}:linux-beta\u0026#34; I recommend that you look at the Butler Docs for the different channels you can push to. To recap here are both complete scripts:\nFinished Scripts 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #!/bin/bash INPUT=\u0026#34;./MyProject\u0026#34; OUTPUT=\u0026#34;./build\u0026#34; RPGMAKER=\u0026#34;M:\\\\SteamLibrary\\\\steamapps\\\\common\\\\RPG Maker MV\u0026#34; npx rpgmpacker@latest \\ --input \u0026#34;$INPUT\u0026#34; \\ --output \u0026#34;$OUTPUT\u0026#34; \\ --rpgmaker \u0026#34;$RPGMAKER\u0026#34; \\ --exclude \\ --noempty \\ --debug \\ --platforms \u0026#34;Windows\u0026#34; \u0026#34;OSX\u0026#34; \u0026#34;Linux\u0026#34; for platform in $OUTPUT/*; do 7z a -tzip -o$OUTPUT $platform.zip $platform/* done # see https://broth.itch.ovh/butler ButlerChannel=\u0026#34;windows-amd64\u0026#34; # darwin-amd64 for Mac and linux-amd64 for Linux ButlerVersion=\u0026#34;15.21.0\u0026#34; ButlerOutput=\u0026#34;butler-$ButlerVersion-$ButlerChannel\u0026#34; if [ ! -d $ButlerOutput ]; then curl -L -o butler.zip https://broth.itch.ovh/butler/$butlerChannel/$butlerVersion/archive/default unzip -o butler.zip -d $ButlerOutput rm butler.zip else echo \u0026#34;Butler version $ButlerVersion ($ButlerChannel) has already been downloaded\u0026#34; fi # change butler.exe to just butler if you are on Unix butler=\u0026#34;$ButlerOutput/butler.exe\u0026#34; export BUTLER_API_KEY=\u0026#34;YOUR_KEY\u0026#34; ButlerProject=\u0026#34;YOUR_USERNAME/YOUR_PROJECT_NAME\u0026#34; ./$butler login ./$butler push $OUTPUT/Windows $ButlerProject:windows-beta ./$butler push $OUTPUT/OSX $ButlerProject:osx-beta ./$butler push $OUTPUT/Linux $ButlerProject:linux-beta 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 $PROJECT=\u0026#34;./MyProject\u0026#34; $OUTPUT=\u0026#34;./build\u0026#34; $RPGMAKER=\u0026#34;M:\\\\SteamLibrary\\\\steamapps\\\\common\\\\RPG Maker MV\u0026#34; npx rpgmpacker@latest ` --input $PROJECT ` --output $OUTPUT ` --rpgmaker $RPGMAKER ` --exclude ` --noempty ` --debug ` --platforms \u0026#34;Windows\u0026#34; \u0026#34;OSX\u0026#34; \u0026#34;Linux\u0026#34; $Platforms = Get-ChildItem -Path $OUTPUT -Directory -Name foreach ($platform in $Platforms) { 7z a -tzip -o$OUTPUT $OUTPUT/$platform.zip $OUTPUT/$platform/* } # see https://broth.itch.ovh/butler $ButlerChannel=\u0026#34;windows-amd64\u0026#34; # darwin-amd64 for Mac and linux-amd64 for Linux $ButlerVersion=\u0026#34;15.21.0\u0026#34; $ButlerOutput=\u0026#34;butler-$ButlerVersion-$ButlerChannel\u0026#34; if (![System.IO.Directory]::Exists($ButlerOutput)) { Invoke-WebRequest -OutFile butler.zip \u0026#34;https://broth.itch.ovh/butler/$butlerChannel/$butlerVersion/archive/default\u0026#34; Expand-Archive -Path butler.zip $ButlerOutput Remove-Item -Path butler.zip } else { Write-Output \u0026#34;Butler version $ButlerVersion ($ButlerChannel) has already been downloaded\u0026#34; } # change butler.exe to just butler if you are on Unix $butler=\u0026#34;$ButlerOutput/butler.exe\u0026#34; $env:BUTLER_API_KEY=\u0026#34;YOUR_KEY\u0026#34; $ButlerProject=\u0026#34;YOUR_USERNAME/YOUR_PROJECT_NAME\u0026#34; \u0026amp;$butler login \u0026amp;$butler push $OUTPUT/Windows.zip \u0026#34;${ButlerProject}:windows-beta\u0026#34; \u0026amp;$butler push $OUTPUT/OSX.zip \u0026#34;${ButlerProject}:osx-beta\u0026#34; \u0026amp;$butler push $OUTPUT/Linux.zip \u0026#34;${ButlerProject}:linux-beta\u0026#34; Conclusion I hope this blog post helped you with getting a CI/CD pipeline up and running for your RPG Maker game. Please let me know if you end up using this tool for your next project. The author of Star Knightess Aura (NSFW) was one of the earliest users, and you can find their pipeline in this repo on gitgud.\nUnreal Engine has Automation Tools and Unity has a CLI\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://erri120.github.io/posts/2021-02-04/","summary":"Learn how to automate content delivery for your RPG Maker game.","title":"CI/CD for RPG Maker Game Development"}]